[{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation\n Online courses\n Tutorials\n  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536444000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536444000,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation\n Online courses\n Tutorials\n  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":["R","Time Series Analysis"],"content":"  Anomalous diffusion: Individual particle tracking vs. Ensemble averages Particles are individuals too!  A timeseries of daily mood: down Test for level and trend stationarity Test for AR, ARCH or an optimal ARIMA process Structural decomposition Conclusions NEXT: Dynamics in state space    Anomalous diffusion: Individual particle tracking vs. Ensemble averages A recent article by Bos et al. (2017)1 in which the authors compared network measures derived from cross-sectional (between-individual) and within-individual data sparked some discussion on facebook.\nThe authors conclude that the outcomes of network analyses based on time-ordered ensemble averages are not the same as the outcomes based on the time evolution of individual trajectories.\nI argued this result is to be expected, because I think the data-generating process underlying time and trial series of human physiology and performance should be considered a non-ergodic process and the particular analyses used to construct the graphical model (the Gaussian Graphical Model) assume ergodic processes. Simply put, the ergodic assumption is that the space-averaged behaviour of some variable observed in an ensemle will be the same as the time-averaged behaviour observed in an individual (… in the limit of infite time and space). If it is violated, this is a problem for such models, because it implies the statistical properties of the process change over time and one cannot depend (fullly) on universal laws of probability to predict or infer properties and behaviour of a system.\nThis post originated as a short tutorial on analysing stationarity and other properties of time series, the current text is an expansion of the page kindly tweeted by Eiko Fried:\nAdded to the tutorials section of psych-networks. If you have any other tutorials that would fit please let me knowhttps://t.co/zgJutWpOs8\n\u0026mdash; Eiko Fried (@EikoFried) May 19, 2017  Particles are individuals too! I often use the Complex Sysems argument to suggest measurements should be considered non-ergodic (many interactions between processes across multiple scales that are mostly multiplicative instead of additive), however, it turns out that studies of individual particle tracking (both biological and physical ‘particles’) also have to deal with ergodicity-breaking, ageing and non-stationarity. This behaviour is called anomalous diffusion, which turns out to be more common in nature than the classical normal diffusion processes studied by Brown, Einstein and many other prominent scientists.\nThe following quotes are from the article Anomalous diffusion models and their properties: non-stationarity, non-ergodicity, and ageing at the centenary of single particle tracking which provides an excellent review and perspective.2 The authors list the conditions for normal diffusion, they should sound familiar because these assunmptions are the same as for most statistical analyses used in the social and life sciences:\n The conditions assumed by Einstein in his derivation of the diffusion equation are (i) the independence of individual particles, (ii) the existence of a sufficiently small time scale beyond which individual displacements are statistically independent, and (iii) the property that the particle displacements during this time scale correspond to a typical mean free path distributed symmetrically in positive or negative directions.\n So, we need:\n Independence of indivdual measurement objects in a sample Memorylessness regarding any interactions between individuals and their environment. The after-effects of interactions should be short-lived and not affect behaviour in the long run (no long-range correlations). Randomly distributed deviations from central tendency (Gaussian error distribution).  What happens when we violate these assumptions?\n In anomalous diffusion processes, at least one of these fundamental assumptions is violated, and the strong convergence to the Gaussian according to the central limit theorem broken. In particular, by departing from one or more of the assumptions (i)–(iii), we find that there exist many different generalisations of the Einstein–Smoluchowski diffusion picture.\n Ok, but what does that mean?\n The fact that we consider this large range of anomalous diffusion processes is the non-universal nature of anomalous diffusion itself. Once we leave the realm of Brownian motion, we lose the confines of the central limit theorem forcing the processes to converge to the Gaussian behaviour predicted by Einstein.\n[…]\nQuite commonly such analyses of time series from experiment or simulations are performed in terms of time averaged observables, in particular, the time averaged MSD [Mean squared displecement = Mean squared ‘error’]. We point out that the physical interpretation of the obtained behaviour of such time averages in terms of the typically available ensemble approaches may be treacherous : many of the anomalous diffusion processes discussed herein lead to a disparity between the ensemble and the time averaged observable, for instance, between the ensemble and time averaged MSDs […] even in the limit of long measurement times. Moreover, it turns out that individual results for time averages […] appear to be irreproducible, despite long measurement times.\n To summarise, when non-ergodicity, non-stationarity and ageing play a role in the timeseries measurements of variables of interest, one cannot rely on ensemble statistics to yield valid inferences, in fact they should be considered treacherous, and individual time series averages are irreproducible.\nWhat follows are some tests that have been developped (in different scientific disciplines) to check assumptions of non-stationarity (and more).\nThis is by no means an exhaustive account of tests (or even a strategy that I would use myself, but that is for another post).\n   A timeseries of daily mood: down Unzip and load these interesting ESM data provided by:\n Kossakowski, J. J., Groot, P. C., Haslbeck, J. M., Borsboom, D., \u0026amp; Wichers, M. (2016, December 12). Data from “Critical Slowing Down as a Personalized Early Warning Signal for Depression.” Retrieved from osf.io/j4fg8\n There is a timecode in the dataset indicating when the participant was promted to nswer some questions. Here, we use it to create a time series object and plot it.\nlibrary(lubridate) beep \u0026lt;- dmy_hms(paste0(df.ts$date,\u0026quot; \u0026quot;,df.ts$beeptime,\u0026quot;:00\u0026quot;),tz = \u0026quot;Europe/Amsterdam\u0026quot;) y \u0026lt;- xts(df.ts$mood_down, order.by = beep) plot(y, main = \u0026quot;Time series of variable \u0026#39;mood_down\u0026#39;\u0026quot;)  Test for level and trend stationarity # Load some Time Series libraries library(nonlinearTseries) library(randtests) library(tseries) library(TSA) # BARTELS RANK TEST [rank version of von NEUMANN\u0026#39;s Ratio Test for Randomness] # H0: randomness # H1: nonrandomness randtests::bartels.rank.test(df.ts$mood_down, alternative = \u0026quot;two.sided\u0026quot;) \u0026gt; \u0026gt; Bartels Ratio Test \u0026gt; \u0026gt; data: df.ts$mood_down \u0026gt; statistic = -16.606, n = 1474, p-value \u0026lt; 2.2e-16 \u0026gt; alternative hypothesis: nonrandomness  # H0: randomness # H1: trend randtests::bartels.rank.test(df.ts$mood_down, alternative = \u0026quot;left.sided\u0026quot;) \u0026gt; \u0026gt; Bartels Ratio Test \u0026gt; \u0026gt; data: df.ts$mood_down \u0026gt; statistic = -16.606, n = 1474, p-value \u0026lt; 2.2e-16 \u0026gt; alternative hypothesis: trend  # H0: randomness # H1: systematic oscillation randtests::bartels.rank.test(df.ts$mood_down, alternative = \u0026quot;right.sided\u0026quot;) \u0026gt; \u0026gt; Bartels Ratio Test \u0026gt; \u0026gt; data: df.ts$mood_down \u0026gt; statistic = -16.606, n = 1474, p-value = 1 \u0026gt; alternative hypothesis: systematic oscillation  # COX-STUART SIGN TEST # H0: randomness # H1: upward or downward trend randtests::cox.stuart.test(na.exclude(df.ts$mood_down), alternative = \u0026quot;two.sided\u0026quot;) \u0026gt; \u0026gt; Cox Stuart test \u0026gt; \u0026gt; data: na.exclude(df.ts$mood_down) \u0026gt; statistic = 246, n = 362, p-value = 7.196e-12 \u0026gt; alternative hypothesis: non randomness  # H0: randomness # H1: upward trend randtests::cox.stuart.test(na.exclude(df.ts$mood_down), alternative = \u0026quot;right.sided\u0026quot;) \u0026gt; \u0026gt; Cox Stuart test \u0026gt; \u0026gt; data: na.exclude(df.ts$mood_down) \u0026gt; statistic = 246, n = 362, p-value = 3.598e-12 \u0026gt; alternative hypothesis: increasing trend  # H0: randomness # H1: downward trend randtests::cox.stuart.test(na.exclude(df.ts$mood_down), alternative = \u0026quot;left.sided\u0026quot;) \u0026gt; \u0026gt; Cox Stuart test \u0026gt; \u0026gt; data: na.exclude(df.ts$mood_down) \u0026gt; statistic = 246, n = 362, p-value = 1 \u0026gt; alternative hypothesis: decreasing trend  # KWIATKOWSKI-PHILLIPS-SCHMIDT-SHIN UNIT ROOT TEST [\u0026#39;reversed\u0026#39; DICKY-FULLER TEST] # H0: level stationarity # H1: unit root [not level stationary] tseries::kpss.test(na.exclude(df.ts$mood_down), lshort = TRUE, null = \u0026quot;Level\u0026quot;) \u0026gt; \u0026gt; KPSS Test for Level Stationarity \u0026gt; \u0026gt; data: na.exclude(df.ts$mood_down) \u0026gt; KPSS Level = 1.7668, Truncation lag parameter = 7, p-value = 0.01  # H0: trend stationarity # H1: not trend stationary tseries::kpss.test(na.exclude(df.ts$mood_down), lshort = TRUE, null = \u0026quot;Trend\u0026quot;) \u0026gt; \u0026gt; KPSS Test for Trend Stationarity \u0026gt; \u0026gt; data: na.exclude(df.ts$mood_down) \u0026gt; KPSS Trend = 0.090975, Truncation lag parameter = 7, p-value = 0.1  Test for AR, ARCH or an optimal ARIMA process First inspect the partial autocorrelation function to get an idea of the AR order.\npar(mfrow=c(1,2)) acf(df.ts$mood_down, na.action = na.pass) pacf(df.ts$mood_down, na.action = na.pass) One could conclude there’s at least \\(AR(3)\\) involved, however there are also possible long-range correlations in these data. I am not an initiate of the how-to-interpret-(P)ACF-to-ARfiMA-orders-cult, so let’s do some tests!\n# KEENAN 1-DEGREE TEST OF NONLINEARITY # H0: time series follows some AR process # H1: time series cannot be considered some AR process TSA::Keenan.test(na.exclude(df.ts$mood_down)) \u0026gt; $test.stat \u0026gt; [1] 5.359887 \u0026gt; \u0026gt; $p.value \u0026gt; [1] 0.02074511 \u0026gt; \u0026gt; $order \u0026gt; [1] 16 Not an AR process, how about ARCH or a best fitting ARiMA?\n# MCLEOD-LI TEST FOR CONDITIONAL HETEROSCEDASTICITY # H0: time series follows some AR process # H1: time series cannot be considered some ARCH process TSA::McLeod.Li.test(y = df.ts$mood_down, plot = TRUE, omit.initial = TRUE)  # MCLEOD-LI TEST FOR CONDITIONAL HETEROSCEDASTICITY # H0: time series follows some AR process # H1: time series cannot be considered some ARiMA process TSA::McLeod.Li.test(object = forecast::auto.arima(df.ts$mood_down), plot = TRUE, omit.initial = TRUE) This test finds no convincing evidence of the timeseries being either ARCH or ARiMA.\n Structural decomposition It is also possible to decompose the time series in different sources of variation. Let’s calculate the average frequency between beeps (in hours), and use that as a guess for any periodicity that may occur in the timeseries.\n(beepMean \u0026lt;- mean(time_length(diff(beep), unit = \u0026quot;hours\u0026quot;))) \u0026gt; [1] 3.886332 ts0 \u0026lt;- ts(df.ts$mood_down, frequency = round(beepMean)) fit\u0026lt;-StructTS(ts0, type=\u0026quot;BSM\u0026quot;) par(mfrow = c(4, 1)) # to give appropriate aspect ratio for next plot. plot(cbind(fitted(fit), resids=resid(fit)), main= \u0026quot;Basic Structural Model for \u0026#39;mood_down\u0026#39;\u0026quot;) There is a lot going on here, the residuals and level fluctuations show we probably should not assume a linear ergodic stochastic change process.\n Conclusions This timeseries is:\n Not level stationary Not random Not oscillatory Not generated by some AR process Not ARCH or generated by a best fitting ARiMA process Contains long range correlations in PACF, e.g. lags 6, 9, 13, 25 and beyond (remember it is 25 * 4 hours!) Upward Trend (which is stationary)  This is likely a violation of the strong ergodic condition / strong memorylessness property:\nThe statistical properties of a between-individual cross-sectional sample of the same theoretical process will be very different from a within-individual sample in a non-trivial way if the process can be considered to be some form of anomalous diffusion.\n So, do not use any analytic techniques that depend on the ESM data and underlying process to be stationary and ergodic if these tests indicate the assumptions are violated.\nNEXT: Dynamics in state space What can we do to study non-ergodic processes?\nAgain from the anomalous diffussion article:\n We note that non-ergodicity […] is not restricted to the spatial diffusion of particles, but similar principles hold for certain processes revealing non-exponential dynamics, such as the blinking behaviour of individual quantum dots or laser cooling. To physically interpret such measurements we need to understand the time averages of individual time series. As we will see, this requires information beyond the conventional ensemble averages for a variety of anomalous diffusion processes.\n This resonates with some great advice from Peter Molenaar in Todd Roses’s book The End of Average:\n “first analyse, then aggregate”\n When I find the time I’ll post some examples of how to represent and quantify state space dynamics of individual time series.\nFor now, the plots below should make clear the occurrence of state stransitions is not distributed according to a uniform or Gaussian pdf.\nlibrary(ggTimeSeries) df.ts2 = data.frame( Signal = round(head(df.ts$mood_down, -1),0), NextSignal = round(tail(df.ts$mood_down, -1),0), Weight = 1, Label = \u0026quot;mood_down\u0026quot;, Size = 0, Time = factor(round(seq(1,150, length.out = max(index(head(beep,-1)))))) ) N = length(df.ts2$Signal) set.seed(123) dfcat \u0026lt;- round(runif(N,min=-3,max=3)) dfcat2 \u0026lt;- data.frame( Signal = round(head(dfcat, -1),0), NextSignal = round(tail(dfcat, -1),0), Weight = 1, Label = \u0026quot;random_uniform\u0026quot;, Size=0, Time = factor(round(seq(1,150, length.out = max(index(head(beep,-2)))))) ) dftot \u0026lt;- rbind(dfcat2,df.ts2) ggplot(dftot, aes(xbucket = Signal, ybucket = NextSignal, fill = factor(NextSignal), weight = Weight) )+ stat_marimekko(color = \u0026#39;black\u0026#39;, xlabelyposition = -0.1) + scale_y_continuous(\u0026quot;Relative frequency\u0026quot;) + scale_x_continuous(\u0026quot;Current value\u0026quot;) + scale_fill_discrete(\u0026quot;Next value\u0026quot;) + facet_wrap(~Label) + ggtitle(label = \u0026quot;State tranisitions: Random numbers vs. \u0026#39;I feel down\u0026#39;\u0026quot;,subtitle = paste(\u0026quot;box dimensions represent relative frequencies of\u0026quot;,N,\u0026quot;values\u0026quot;)) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), strip.background = element_rect(fill=\u0026quot;grey90\u0026quot;), strip.text = element_text(face = \u0026quot;bold\u0026quot;), plot.background = element_blank(), panel.border = element_blank(), panel.background = element_blank(), panel.grid = element_blank())  If we consider the values \\(-3\\) to \\(3\\) to span the possibe states the ‘system’ can be in, then a crude way to study the dynamics of the states is to look at a lag-1 return plot.\nlibrary(gganimate) tmp \u0026lt;- as.data.frame(table(df.ts2$Signal,df.ts2$NextSignal)) # Set the size to frequency of occurrence of each combination of values for(c in seq_along(tmp$Freq)){ ID \u0026lt;- (df.ts2$Signal%in%tmp$Var1[c])\u0026amp;(df.ts2$NextSignal%in%tmp$Var2[c]) if(sum(ID)\u0026gt;0){ df.ts2$Size[ID] \u0026lt;- tmp$Freq[c] } } ga \u0026lt;- ggplot(df.ts2,aes(x=Signal,y=NextSignal, frame = Time)) + geom_path(colour = \u0026quot;grey80\u0026quot;) + geom_point(aes(size=Size)) + scale_y_continuous(\u0026quot;Next value\u0026quot;) + scale_x_continuous(\u0026quot;Current value\u0026quot;) + scale_size_continuous(\u0026quot;Frequency of\\n pair (Curren,Next)\u0026quot;) + ggtitle(label = \u0026quot;Window\u0026quot;, subtitle = \u0026quot;Return plot\u0026quot;) + theme_bw() + coord_equal() gganimate(ga, interval=.2, \u0026quot;returnplot.gif\u0026quot;)   It is appears to be the case the system is attracted to specific states. Perhaps the dynamics can be described as transitions between more or less stable states, or stable temporal patterns of recurring states.\nto be continued …\n天\n   Bos, F. M., Snippe, E., de Vos, S., Hartmann, J. A., Simons, C. J., van der Krieke, L., \u0026amp; Wichers, M. (2017). Can We Jump from Cross-Sectional to Dynamic Interpretations of Networks? Implications for the Network Perspective in Psychiatry. Psychotherapy and Psychosomatics, 86(3), 175-177↩\n Metzler, R., Jeon, J. H., Cherstvy, A. G., \u0026amp; Barkai, E. (2014). Anomalous diffusion models and their properties: non-stationarity, non-ergodicity, and ageing at the centenary of single particle tracking. Physical Chemistry Chemical Physics, 16(44), 24128-24164.↩\n   ","date":1495212602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495212602,"objectID":"2d887e3b3f9832ccf546c0dfc857b66f","permalink":"/post/2017-05-19-testing-assumptions-of-the-data-generating-process-underlying-experience-sampling/","publishdate":"2017-05-19T09:50:02-07:00","relpermalink":"/post/2017-05-19-testing-assumptions-of-the-data-generating-process-underlying-experience-sampling/","section":"post","summary":"Anomalous diffusion: Individual particle tracking vs. Ensemble averages Particles are individuals too!  A timeseries of daily mood: down Test for level and trend stationarity Test for AR, ARCH or an optimal ARIMA process Structural decomposition Conclusions NEXT: Dynamics in state space    Anomalous diffusion: Individual particle tracking vs. Ensemble averages A recent article by Bos et al. (2017)1 in which the authors compared network measures derived from cross-sectional (between-individual) and within-individual data sparked some discussion on facebook.","tags":["Ergodicity","Structural Decomposition","Homogeneity","Stationarity","Anomalous Diffusion"],"title":"Testing assumptions of the data-generating process underlying Experience Sampling","type":"post"},{"authors":null,"categories":["accidental mathematician"],"content":" Why?\nNo reason, this is the accidental mathematician category.\nPandigital numbers Pandigital numbers aren’t that special, it’s a number that has all digits from 1-9, or, 0-9 in it.\nFor example: 381654729 is a pandigital number (with some extra properties)\n  Pandigital functions As Complexity Scientists (and musicians and artists in general) know:\n Interesting things will happen when the degrees of freedom available to a system to generate its behaviour, are reduced.\n Now, the pandigital constraint can become very interesting when applied to functions, or rather, mathematical operators. The rules are:\n Take a pandigital number Stick any mathematical operator between any cluster of digits Attempt to get an interesting outcome  Turns out such a pandigital formula is able to approximate the trancendental number \\(e\\) with uncanny precision!\n\\[e\\approx\\left (1 + 9^{-4^{6*7}} \\right)^{3^{2^{85}}}\\]\nIt was discovered by Richard Sabey in 2004.\nThe Numberphile channel has a great video on it:\nHUGOMORE42   天\n ","date":1494348602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494348602,"objectID":"200ce28ef866c527a6a7b0dc914ce937","permalink":"/post/2017-05-09-pandigital-numbers-and-functions/","publishdate":"2017-05-09T09:50:02-07:00","relpermalink":"/post/2017-05-09-pandigital-numbers-and-functions/","section":"post","summary":"Why?\nNo reason, this is the accidental mathematician category.\nPandigital numbers Pandigital numbers aren’t that special, it’s a number that has all digits from 1-9, or, 0-9 in it.\nFor example: 381654729 is a pandigital number (with some extra properties)\n  Pandigital functions As Complexity Scientists (and musicians and artists in general) know:\n Interesting things will happen when the degrees of freedom available to a system to generate its behaviour, are reduced.","tags":["pandigital function","e"],"title":"Reproduce $e$ to $18,457,734,525,360,901,453,873,570$ decimal places!","type":"post"},{"authors":null,"categories":null,"content":" A beginning is the time for taking the most delicate care that the balances are correct.\n\u0026ndash; Fom Manual of Muad’Dib by Prinses Irulan (Herbert, 1965)\n It\u0026rsquo;s been a while\u0026hellip;\nIf you still have a while\u0026hellip;\nWatch / listen to this excellent display of intelligent behaviour:\n  天\n","date":1488584102,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488584102,"objectID":"0589222ba3d1b6a5d953f8b3e13362bd","permalink":"/post/2017-03-02-oh-right-i-used-to-blog/","publishdate":"2017-03-03T16:35:02-07:00","relpermalink":"/post/2017-03-02-oh-right-i-used-to-blog/","section":"post","summary":" A beginning is the time for taking the most delicate care that the balances are correct.\n\u0026ndash; Fom Manual of Muad’Dib by Prinses Irulan (Herbert, 1965)\n It\u0026rsquo;s been a while\u0026hellip;\nIf you still have a while\u0026hellip;\n","tags":["joe rogan","neil degrasse tyson","don't debate...educate!"],"title":"Oh right... I used to blog","type":"post"},{"authors":null,"categories":["R","HIBAR"],"content":" Using R to Compute Effect Size Confidence Intervals This is a demonstration of using R in the context of hypothesis testing by means of Effect Size Confidence Intervals. In other words, we’ll calculate confidence intervals based on the distribution of a test statistic under the assumption that \\(H_0\\) is false, the noncentral distribution of a test statistic.\nDiscrete distributions: Count data and proportions  Discrete CI ≠ Approximately Continuous CI Noncentral CIs around the Odds Ratio of Fisher’s Exact Test Example: Time, Money, and Morality  Continuous distributions: Standardised sample means  Standardised, but on which measure of dispersion? Example: Neural reactivation links unconscious thought to decision-making performance  Learn From Reproducible Open Data  Example: The ManyLabs project    **Notes:**\nNew to R? Here are some pointers to get you started\n # Source a file from Github to get function in.IT(), which will load and, if necessary, install packages passed as a list. require(devtools) source_url(\u0026#39;https://raw.githubusercontent.com/FredHasselman/toolboxR/master/C-3PR.R\u0026#39;) in.IT(c(\u0026quot;MBESS\u0026quot;,\u0026quot;metafor\u0026quot;,\u0026quot;plyr\u0026quot;))  Discrete distributions: Count data Discrete CI ≠ Approximately Continuous CI Many studies in the social sciences compare variables across the levels of a design factor that are discrete in nature. These discrete variables often represent countable units: Participants, behaviours, correctly answered items, etc. At the aggregate level a proportion or percentage is often the object of analysis. It is common practice to disregard the discrete nature of the variable under investigation and assign probabilities to observed events using a continuous distribution function given large enough sample sizes. For example, the test for equality of sample proportions uses the standardised normal distribution (Z-score). In R this test can be conducted using the function prop.test, by default, it performs Yates’ continuity correction. Depending on the situation such corrections can be either too lenient or too strict, moreover, calculating a symmetrical, continuous CI around a discrete variable is almost always inaccurate.\nLet’s review some properties of discrete probability distributions: A distribution function that assigns a probability to the value taken on by a random variable due to the occurrence of a discrete random event is called a Probability Mass Function. They come in several flavours, the binomial distribution function assigns probabilities to events drawn from a finite population with replacement. The hypergeometric distribution function assigns probabilities to events drawn without replacing them, and the poisson distribution describes independent event probabilities based on an expected value of ‘successes’ denoted as \\(\\lambda\\).\nThese probability mass functions and their cumulative distribution function are shown in the figure below, the value with highest mass is 3 in all cases (the code to generate these figures is available in the .Rmarkdown file that generated this page).\nIt is evident from the figures above that for small sample sizes the CI based on a discrete distribution is not symmetrical: These distributions are zero inflated and depending on the context, can have fat tails when large samples are concerned.\nAnother reason to look for a more accurate CI around a discrete ES is a less than ideal analysis strategy that is often used when there are more than 2 factor levels in the design. One often calculates a \\(\\chi^2\\) statistic for the large design table and subsequently interprets \\(\\chi^2\\) tests on 2X2 partitions of the full design as level or group comparisons, like in a linear model. In my opinion this is an incorrect strategy: A \\(\\chi^2\\) test is a goodness-of-fit test and should not be used to test hypotheses about the effects of a linear predictor on a dependent variable. One should use a generalised linear model with a poisson link (poisson regression, log-linear analysis), or binomial link (logistic, probit regression) function to answer such questions.\nWhat options are available if one does not want to embark on a generalised mixed model fitting spree?\n Noncentral CIs around the Odds Ratio of Fisher’s Exact Test The solution is in fact quite simple when you are using R. No extra packages are required, because the fisher.test() function available in the core stats package will give you exactly the effect size you need: An exact Odds Ratio (OR) with CIs based on the noncentral hypergeomtric distribution for 2x2 tables. The function performs Fisher’s exact test, which for 2x2 tables amounts to evaluating \\(H_0: OR = 1\\). If the Odds Ratio is 1, the cell counts are conditionally independent of row and column variables and one would infer: “There is no effect”.\nYou’ve probably read somewhere that Fisher’s Exact Test can take a long time to compute when the table is very large. That is correct, but when examples of large datasets with discrete variables in recent experimental studies are considered, I would not worry too much about the computation time. Moreover, R only gives the noncentral CIs for 2x2 tables, therefore this specific strategy can only be applied in the following common analysis context:\nWhen dealing with tables larger than 2x2, check dependence of row and column variables using whatever test is convenient to do so.\n When it is appropriate to analyse whether variation in cell counts depends on the levels a row or column factor, use Fisher’s exact test on the appropriate 2x2 sub-tables to get an exact Odds Ratio (OR).\n Adjust the confidence level of the of the confidence interval around the OR estimate to take into account the number of statistical tests that were conducted on a partition of the full table: \\(CL = \\frac{(1-\\alpha)}{n_{comparisons}}\\)   Note: The \\(OR\\) is usually expressed as \\(\\log(OR)\\)\n  Example: Time, Money, and Morality This example is taken from: A Post-Publication Peer-Review (3PR) of Time, Money, and Morality, based on: Gino et al., 2013\nThis study examines unethical behaviour in participants as a function of induced states of self-interest or self-reflection. One of the dependent variables is the observation of cheating behaviour in participants, in order to receive a higher financial reward for completing the experiment. The two states are induced by subconscious priming of the concept of Money and Time respectively. In addition to subconscious priming, 2 of the 4 studies also used inductions of self-reflection by means of framing the task as a personality vs. intelligence assessment, or by placing a mirror in the experimentation room.\nSetup the data\nThe R code below creates 4 tables based on the published results of the 4 experiments (however, see link to HIBAR above for details). The columns are Cheating=YES and Cheating=NO, and rows are the conditions in each experiment.\nAfter each table is assigned cell counts (and row and column names), the variable name is called. Normal R behaviour is to display the contents of the variable.\n# Experiment 1 - Cheating(YES,NO) X Prime(Money,Time,No Prime): table.1 \u0026lt;- as.table(cbind(c(28,14,22),c(4,19,11))) dimnames(table.1) \u0026lt;- list(Test=c(\u0026quot;Money\u0026quot;,\u0026quot;Time\u0026quot;,\u0026quot;Control\u0026quot;), Cheating=c(\u0026quot;CheatYES\u0026quot;,\u0026quot;CheatNO\u0026quot;)) table.1 \u0026gt; Cheating \u0026gt; Test CheatYES CheatNO \u0026gt; Money 28 4 \u0026gt; Time 14 19 \u0026gt; Control 22 11 Setup the analysis\nThe statistical hypotheses concern comparisons of individual conditions, in ESCI terms, one needs an effect size with confidence interval for multiple 2x2 partitions of the full design table. In experiment 1 \u0026amp; 4 the number of 2x2 sub-tables that are possible is 3 and in experiment 2 \u0026amp; 3 there are 4 such partitions of the full table. In order to take into account that we are questioning a random sample more than once, a correction of the \\(\\alpha\\) level is appropriate and this implies a CI of 0.9833333 for experiment 1.\nThe 2x2 partitions are easy to create in R. For instance, to compare row 1 and 3 of table.1 created above one can us the following:\n# Subset a table by calling specific row numbers: [c(1,3),1:2] # Omitting column IDs [c(1,3),] will return all columns table.1[c(1,3),] \u0026gt; Cheating \u0026gt; Test CheatYES CheatNO \u0026gt; Money 28 4 \u0026gt; Control 22 11  Note: It often pays off to take the time to create sensible row and column labels; it is immediately clear which comparison is up for analysis.\n The function fisher.test() can be called using this syntax and we need just one extra argument which is the desired confidence level. The code below clearly contains repetitions, the test is conducted 14 times! Experienced R programmers will shake their head and maybe even shed a tear, but the purpose here is to give an idea of what it is we are calculating and how those results are converted into a plot.\n# Create confidence levels adjusted for comparisons CL \u0026lt;- (1-.05/3) # Perform Fisher\u0026#39;s Exact Test on the appropriate 2x2 table c1.1 \u0026lt;- fisher.test(table.1[c(1,2),],conf.level=CL) c1.2 \u0026lt;- fisher.test(table.1[c(1,3),],conf.level=CL) c1.3 \u0026lt;- fisher.test(table.1[c(2,3),],conf.level=CL) The output of the fisher.test() function is a list with different kinds of values, the OR estimates, the CIs and the exact p-values. The code below creates a data frame called ORcomp, it will be used to plot the results.\nSetup the plot\n# Create a data frame ORcomp \u0026lt;- data.frame(cbind(comparison=1:3, log(rbind(c1.1$estimate,c1.2$estimate,c1.3$estimate)), log(rbind(c1.1$conf.int,c1.2$conf.int,c1.3$conf.int)), round(rbind(c1.1$p.value,c1.2$p.value,c1.3$p.value),digits=4))) # Some columns do not have a name yet... names(ORcomp)[3] \u0026lt;- \u0026quot;lo\u0026quot; names(ORcomp)[4] \u0026lt;- \u0026quot;hi\u0026quot; names(ORcomp)[5] \u0026lt;- \u0026quot;p\u0026quot; # Create a factor with labels to indicate which comparison we are dealing with ORcomp$comparison \u0026lt;- factor(ORcomp$comparison,labels=c(\u0026quot;EXP1: Money-Time\u0026quot;,\u0026quot;EXP1: Money-Control\u0026quot;,\u0026quot;EXP1: Time-Control\u0026quot;)) # Show the data frame ORcomp \u0026gt; comparison odds.ratio lo hi p \u0026gt; 1 EXP1: Money-Time 2.212450 0.6510981 4.1467802 0.0002 \u0026gt; 2 EXP1: Money-Control 1.233574 -0.3954537 3.1832615 0.0759 \u0026gt; 3 EXP1: Time-Control -0.982712 -2.3549566 0.3207653 0.0828 A forest (the cure to publication bias?)\nAll that is left is to create a plot of the results. The function forest() from the package metafor is used to create a so called forest plot, the Bonferroni adjusted p-values of the exact test will be added to the plot.\n# Bonferroni adjustment of the exact p-value adj \u0026lt;- c(rep(3,3))*ORcomp$p adj[adj\u0026gt;1] \u0026lt;- 1 # Plot the results stored in data frame ORcomp forest(x=ORcomp$odds.ratio,ci.lb=ORcomp$lo, ci.ub=ORcomp$hi, slab=ORcomp$comparison, ilab=adj, ilab.xpos=5, alim=c(-3.5,4.5), main=\u0026quot;Forest Plot\u0026quot;, xlim=c(-7,9), xlab=\u0026quot;Exact log Odds Ratio\u0026quot;, efac=1, cex=.9, mgp=c(1,1,0)) # Add some text text(7.5,(nrow(ORcomp)+1.5),\u0026quot;log OR [CI.9833]\u0026quot;) text(-4.4,(nrow(ORcomp)+1.5),\u0026quot;Contrasts comparing Cheating frequency\u0026quot;) text(5,(nrow(ORcomp)+1.5),\u0026quot;Adjusted p\u0026quot;) Only the first comparison does not include 0. In the original article, the number of significant contrasts reported in 14 comparisons of occurrence of cheating behaviour conducted in 4 samples, was 9. This was based on a continuous test of sample proportions, without correcting for multiple comparisons. Both in R and in SPSS (for 2x2 tables), this is a deliberate choice, because the default behaviour is to apply Yates’ correction. If the continuous test is continuity corrected and a Bonferroni adjustment is applied, 2 significant results remain in the entire article. The ESCI hypothesis test conducted for all 4 experiments gives the same result: Just 2 intervals remain that do not include log(1)=0.\n  Continuous distributions: Standardised sample means Standardised, but on which measure of dispersion? When continuous variables are compared across different independent samples, the effect size of interest is often the standardised mean difference, also known as \\(\\text{Cohen\u0026#39;s d} = \\frac{\\bar{X_1}-\\bar{X_2}}{s}\\). One source of confusion that can arise when comparing this effect size between studies, is that different dispersion measures may be used to standardise the mean difference. In the equation, \\(s\\) can mean at least two things: A reference sample’s standard deviation (e.g. a control group), or a pooled standard deviation.\nIt is of course important to know which dispersion measure was used to calculate Cohen’s d in order to build a CI around it. The R package MBESS contains separate functions to deal with each situation:\nci.sm() Confidence interval for the standardised mean: \\(\\frac{\\bar{X}-\\mu}{SD_X}\\)\n ci.smd() Confidence interval for the standardised mean difference: \\(\\frac{\\bar{X_1} - \\bar{X_2}}{SD_{pooled}}\\)\n ci.smd.c() Confidence interval for the control standardised mean difference: \\(\\frac{\\bar{X_C}-\\bar{X_E}}{SD_C}\\)  Most MBESS functions allow different sets of input arguments to calculate the CIs:\nBased on sample descriptives:  ci.sm(Mean = , SD = , N = ), or: ci.sm(sm = , N = ) ci.smd(smd = , n.1 = , n.2 = ) ci.smd.c(smd.c = , n.C = , n.E = )  Based on the estimated non-centrality parameter (value of the test statistic, most often Student’s t assuming homogeneity of variance):  ci.sm(ncp = , N = ) ci.smd(ncp = , n.1 = , n.2 = ) ci.smd.c(ncp = , n.C = , n.E = )   The confidence interval coverage (\\(1 - \\alpha\\)) is .95 by default, this can be adjusted:\nSymmetrically, e.g., ci.sm(sm = 2.1, N = 15, conf.level = .99) Asymmetrically, separate \\(\\alpha\\) for lower and upper bound of the CI, e.g., ci.smd(ncp = 3.3, n.1 = 20, n.2 = 19, alpha.lower = 0, alpha.upper = .05)   Example: Neural reactivation links unconscious thought to decision-making performance A study by Creswell et al. (2013) examined whether it could find neural correlates of the so-called Unconscious Thought Effect (cf. Dijksterhuis, 2013)\nThe claim is that incubation effects, the unreflective emergence of meaning that is associated with creative processes and insight in problem solving, also occur in complex decision making. Participants who are distracted from thinking about a decision problem (UT) make a better choice than participants who had a chance to consciously think about the problem (CT), or who had to make an immediate decision after the problem was presented (ID).\nThis paradigm was implemented as a within-subjects fMRI study in which participants went through all 3 conditions rating each one of three Cars, Apartments, Backpacks. The accuracy of the decision process was defined as the difference between the rating for the best item and the worst item. Of course, for the fMRI data to make any sense, it is important that the behavioural effects are replicated. From the article:\n Paired t-tests indicated that UT produced better decisions compared with the ID [\\(t(26)=2.15, P=0.04\\)] and CT [\\(t(26)=2.10, P=0.04\\)] conditions [the overall one-way ANOVA was marginally significant, \\(F(2,52)=2.48, P=0.09\\)].\n […]\n In this study, we did not find that a 2-min period of CT produced better decision making compared with the ID condition [paired samples \\(t(26)=0.20, P=0.84\\)]\n Note that a one-way ANOVA is not the correct analysis for repeated measures data, oddly enough the df of the F-test, F(2,52) do not correspond to a one-way ANOVA at all. That would have been F(2,24) and for a repeated measures ANOVA F(2,50). In any case, df and p-value were apparently ‘close enough’ to warrant three post-hoc paired samples t-tests. Indeed, use of paired sample tests is correct here, but a correction for multiple comparisons should be used and at \\(\\alpha = 0.017\\) none of these post-hoc tests is significant. This is in accordance with the F-test for the main effect.\nIn addition to the behavioural data, the authors report tests that show neural reactivation in several areas of the brain predicts decision performance (difference between judgements of the best and the worst item). The association between neural reactivation and performance was not tested in one model even though the contrasts are just linear combinations of the within-subject design factor and neural activity and rating differences are measured from one and the same random outcome, the randomly selected participant.\n We observed that neural reactivation occurring in right dorsolateral PFC [\\(\\beta=0.39, t(26)=2.13, P=0.04\\)] and left intermediate visual cortex [\\(\\beta=0.40, t(26)=2.20, P=0.04\\)] predicted subsequent decision-making performance, such that more neural reactivation in these regions was associated with greater discrimination between the best and worst items on the decision-making task.\n […]\n Although we observed clusters of neural reactivation during CT (Figure 5; Table 4), none of these clusters significantly predicted decision-making performance. Specifically, CT reactivation clusters observed in right cerebellum [\\(\\beta\\)=0.27, t(26)=1.39, P=0.18], left supplementary motor area [\\(\\beta=0.21, t(26)=1.07, P=0.29\\)], right ventrolateral PFC [\\(\\beta=0.18, t(26)=0.90, P=0.38\\)] and right intraparietal lobule [\\(\\beta=0.13, t(26)=0.67, P=0.51\\)] did not predict decision performance after CT.\n T-tests galore! Again, one should accommodate in some way for multiple hypothesis testing and assuming just 2 tests were conducted to evaluate the benefits of UT mode of thought at \\(\\alpha = 0.025\\), we might as well be looking at a dead salmon, or at least a red herring.\nGranted, the neural activity measure used in these tests is itself the result of multiple comparison corrected analyses. This measure is the output from a conjunction analysis in which activity during several different conditions is examined conditional on a specific contrast or hypothesis (UT: (UT n-back task \u0026gt; independent n-back task) AND (encoding \u0026gt; fixation); CT: (CT fixation \u0026gt; fixation) AND (encoding \u0026gt; fixation)). The purpose is to identify clusters of voxels whose activity is connected and associated to the UT and CT modes of thought as specified in the contrast. That is, assuming brain physiology can be neatly decomposed into the architecture of cognitive components and processes posited to exist by these authors, but that is another story.\nHow to deal with within-subjects effect sizes? There must be correlations between within-subject conditions, so is it at all possible to calculate ESCIs? Yes, provided one can be sure the t-value was obtained from a paired samples t-test, because then it is associated to a tests of the mean difference. An effect size in this case is a standardised mean, whose magnitude indicates how much the observed within-subject mean difference deviates from 0. The MBESS function ci.sm(ncp= , N= ) can be used here, taking the paired samples t-value as the non-centrality parameter, ncp.\nSetup the data\nCompared to the OR example, the code below reflects a different approach to get the ESCIs.\nIt is in general more efficient in R to use the so-called apply functions that ship with R when you need to apply the same function repeatedly to different data vectors. These functions take each element of a list object and pass those to a function, returning the results as another list object. A package that can make your apply coding a little easier is plyr, it contains functions that are sometimes just wrappers for the native apply functions, but the great advantage is that the input and output object types can be chosen.\n# Create a list of confidence levels adjusted for the number of comparisons of each family of tests (3, 2 and 4 respectively) CL \u0026lt;- c(rep(1-.05/3,3),rep(1-.05/2,2),rep(1-.05/4,4)) CL \u0026gt; [1] 0.9833333 0.9833333 0.9833333 0.9750000 0.9750000 0.9875000 0.9875000 0.9875000 0.9875000 # This is a list object with t-values from the article as elements. # The elements are named after the contrast / paired observation that was tested. NCP \u0026lt;- list(UTvsID=c(2.15),UTvsCT=c(2.10),CTvsID=c(0.20),UTpfc=c(2.13),UTvzc=c(2.20),CTcbl=c(1.39),CTsma=c(1.07),CTpfc=c(.90),CTlob=c(0.67))  Setup the analysis\nThat’s it, a single line of code!\n# The function `ldply` passes the elements in NCP (t-values) to an anonymous function as `p`. # This ensures the t-values are interpreted as the correct input argument for ci.sm: ncp=p # `ldply` returns the data in a dataframe, native `apply` functions would return a list object. PTcomp \u0026lt;- ldply(seq_along(NCP), function(p) data.frame(test=names(NCP)[p],ci.sm(ncp=NCP[[p]], N=27, conf.level=CL[p]))) Setup the plot\n# Plot the results stored in data frame PTcomp forest(x=PTcomp$Standardized.Mean, ci.lb=PTcomp$Lower.Conf.Limit.Standardized.Mean,ci.ub=PTcomp$Upper.Conf.Limit.Standardized.Mean,slab=PTcomp$test, alim=c(-1.5,2.5), ilab=round(CL,digits=3), ilab.xpos=2.2, main=\u0026quot;Forest Plot\u0026quot;, xlim=c(-3,5), xlab=\u0026quot;Standardised Mean\u0026quot;, efac=1, cex=.9, mgp=c(1,1,0)) # Add some text text(2.2,(nrow(PTcomp)+1.5),\u0026quot;adjusted CL\u0026quot;) text(3.9,(nrow(PTcomp)+1.5),\u0026quot;Standardised Mean [Low,High]\u0026quot;) text(-2.2,(nrow(PTcomp)+1.5),\u0026quot;Dependent samples t-test\u0026quot;)  Neural correlates of… barely observable behavioural phenomena? It is tempting to place more epistemic weight to psychological effects evidenced using expensive measurement equipment, especially if those measurements concern outcomes of observables at the level where biology meets (quantum) physics. In this case, it is especially important not to reverse the chain of evidence: There is no difference between conditions in the behavioural measures which led to doing the experiment in an fMRI scanner! The data should not be presented as a confirmation of the behavioural effect:\n Using BOLD contrast fMRI, we observed neural activity during an UT period, which challenges existing accounts that have claimed that deliberation without attention (UT) does not occur during periods of distraction (Acker, 2008).\n The first part is partly correct, neural activity was observed in UT, but also in CT. The second part is incorrect:\n The deliberation without attention effect concerns an advantage in decision making and this was not observed.\n The fact that activity was observed reveals nothing about the processes that were going on, moreover, the CT condition does not exclude that UT is taking place as well.\n There was no association between neural activation and performance. If anything, the full interpretation concerns an interaction between Mode of Thought and Neural reactivation networks to predict decision performance. This interaction was never tested.  The correct interpretation of the results of the study is: Within an individual, there are different clusters of jointly active (=connected?) voxels, whose level of activity are represented by a t-value resulting from a test of a contrast composed of a set of relational constraints on the BOLD signal measured in different experimental conditions. It was thus shown that combining brain activity from experimental conditions that differ both in task, cognitive load and visual complexity into statistical hypotheses, will reveal that different clusters of voxels are active.\n   Learn From Reproducible Open Data Yes, much more to learn about R, about ESCI, and meta-analysis!\nExample: The ManyLabs project\nA good place to continue from here on is to have a look at the R scripts used in the ManyLabs project.\nThere are three R scripts available from the OSF ManyLabs project pages that will also show you how to read from and write to a spreadsheet and how to save graphics device output to a pdf file:\n1.Manylabs_OriginalStudiesESCI.R - ESCI for the original studies (correlations, \\(\\chi^2\\), Cohen’s d).\n2.Manylabs_ReplicationStudiesESCI.R - ESCI for the replication data (correlations, \\(\\chi^2\\), Cohen’s d).\n3.ManyLabs_Heterogeneity.R - Meta-analysis (forest plot, funnel plot, influence plot, radial plot, heterogeneity measures).\nGood luck!\nFred Hasselman\n New to R? You have probably heard many people say they should invest more time and effort to learn to use the R software environment for statistical computing… and they were right. However, what they probably meant to say is: “I tried it, but it’s so damned complicated, I gave up”… and they were right. That is, they were right to note that this is not a point and click tool designed to accommodate any user. It was built for the niche market of scientists who use statistics, but in that segment it’s actually the most useful tool I have encountered so far. Now that your struggles with getting a grip on R are fully acknowledged in advance, let’s try to avoid the ‘giving up’ from happening. Try to follow these steps to get started:\nGet R and add some user comfort: Install the latest R software and install a user interface like RStudio… It’s all free! An R interface will make some things easier, e.g., searching and installing packages from repositories. RStudio will also add functionality, like git/svn version control, project management and more, like the tools to create html pages like this one (knitr and Rmarkdown). Another source of user comfort are the packages. R comes with some basic packages installed, but you’ll soon need to fit generalised linear mixture models, or visualise social networks using graph theory and that means you’ll be searching for packages that allow you to do such things. A good place to start package hunting are the CRAN task view pages.\n Learn by running example code: Copy the commands in the code blocks you find on this page, or any other tutorial or help files (e.g., Rob Kabacoff’s Quick R). Paste them into an .R script file in the script (or, source) editor. In RStudio You can run code by pressing cmd + enter when the cursor is on a single single line, or you can run multiple lines at once by selecting them first. If you get stuck remember that there are expert R users who probably have answered your question already when it was posted on a forum. Search for example through the Stackoverflow site for questions tagged with R)\n Examine what happens… when you tell R to make something happen: R stores variables (anything from numeric data to functions) in an Environment. There are in fact many different environments, but we’ll focus on the main workspace for the current R session. If you run the command x \u0026lt;- 1+1, a variable x will appear in the Environment with the value 2 assigned to it. Examining what happens in the Environment is not the same as examining the output of a statistical analysis. Output in R will appear in the Console window. Note that in a basic set-up each new R session starts with an empty Environment. If you need data in another session, you can save the entire Environment, or just some selected variables, to a file (.RData).\n Learn about the properties of R objects: Think of objects as containers designed for specific content. One way to characterize the different objects in R is by how picky they are about the content you can assign it. There are objects that hold character and numeric type data, a matrix for numeric data organised in rows and columns, a data.frame is a matrix that allows different data types in columns, and least picky of all is the list object. It can carry any other object, you can have a list of which item 1 is an entire data.frame and item 2 is just a character vector of the letter R. The most difficult thing to master is how to efficiently work with these objects, how to assign values and query contents.\n Avoid repeating yourself: The R language has some amazing properties that allow execution of many repetitive algorithmic operations using just a few lines of code at speeds up to warp 10. Naturally, you’ll need to be at least half Vulcan to master these features properly and I catch myself copying code when I shouldn’t on a daily basis. The first thing you will struggle with are the apply functions. These functions pass the contents of a list object to a function. Suppose we need to calculate the means of column variables in 40 different SPSS .sav files stored in the folder DAT. With the foreign package loaded we can execute the following commands:\ndata \u0026lt;- lapply(dir(\u0026quot;/DAT/\u0026quot;,pattern=\u0026quot;.sav$\u0026quot;),read.spss)\nout \u0026lt;- sapply(data,colMeans)\nThe first command applies read.spss to all files with a .sav extension found in the folder /DAT. It creates a dataframe for each file which are all stored as elements of the list data. The second line applies the function colMeans to each element of data and puts the combined results in a matrix with dataset ID as columns (1-40), dataset variables as rows and the calculated column means as cells. This is just the beginning of the R magic, wait ’till you learn how to write functions that can create functions.\n   References   Creswell, J. D., Bursley, J. K., \u0026amp; Satpute, A. B. (2013). Neural reactivation links unconscious thought to decision-making performance. Social Cognitive and Affective Neuroscience, 8(8), 863–869. doi:10.1093/scan/nst004\n  Dijksterhuis, A. (2013). First neural evidence for the unconscious thought process. Social Cognitive and Affective Neuroscience, 8(8), 845–846. doi:10.1093/scan/nst036\n  Gino, F., \u0026amp; Mogilner, C. (online, 2013). Time, Money, and Morality. Psychological Science. doi: 10.1177/0956797613506438   ","date":1430827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430827200,"objectID":"072792b66f2625dd5c578b22758746e0","permalink":"/post/2015-05-05-osc-r-esci-tutorial/","publishdate":"2015-05-05T12:00:00Z","relpermalink":"/post/2015-05-05-osc-r-esci-tutorial/","section":"post","summary":"This is a demonstration of using `R` in the context of hypothesis testing by means of Effect Size Confidence Intervals.","tags":["HIBAR","3PR","ESCI","Tutorial"],"title":"R Effect Size CI tutorial","type":"post"},{"authors":null,"categories":null,"content":" A Post-Publication Peer-Review (3PR) of: Time, Money, and Morality by Gino, F., \u0026amp; Mogilner, C. (online, 2013). Time, Money, and Morality. Psychological Science. DOI: 10.1177/0956797613506438\n Introduction The Time, Money, and Morality article has been HIBAR-ed on Twitter and the Blogosphere (e.g., by Rolf Zwaan and Greg Francis ) and the discussion seems to revolve around the validity of the inferences based p-values close to 0.05 (e.g., they raise suspicions of p-hacking).\nIn short, the article reports of 4 Experiments testing 2 core postulates:\n* Postulate 1: Priming Money activates self-interest and increases unethical behaviour * Postulate 2: Priming Time activates self-reflection and decreases unethical behaviour\nUnethical behaviour is operationalised as taking the opportunity to cheat on a task.\nPriming methods vary across experiments, so do the tasks that allow for an opportunity to cheat.\nIn Experiment 1 the two postulates are tested, Experiments 2-4 concern an assessment of the role of self-reflection on cheating behaviour and is operationalised differently across experiments.\nHold on to your P-curves for a moment… Back to the basics! In this Post-Publication Peer-Review (3PR) I demonstrate that there is indeed some cause for concern about the way these results are presented and interpreted. Was it p-hacking? … I don’t know and maybe I don’t even care. To me this is an example of sloppy science, p-hacked or not, these results were allowed to be published by expert peers. It is more relevant to discuss the broken system of quality control that should have picked up on at least some of the following issues:\nImportant information is missing:   in general (e.g., number of subjects per condition, sample size determination)\n selectively across experiments (e.g., participants per cell, reporting of effect sizes)\n  The analyses used on frequency data are inappropriate\n Invalid or biased inferences and oddities:\n   No adjustments for multiple comparisons\n “Marginal significance” shifts ad hoc between 0.1 \u0026gt; p \u0026gt; 0.05\n Obvious intervening/mediator variable is omitted: Accuracy of performance\n No explanation of (conflicting) results across experiments (e.g., variation in amount of cheating) No explanation for failing of random assignment to design levels (none of the experiments have equal N samples)  The article under scrutiny is by no means exceptional with respect to such issues, moreover, the way frequency / proportion data are analysed in psychological science is generally awkward and most of the time completely wrong.\nI will 3PR the data based on the information in the article and comment on the results:\nAnalysis of Proportion / frequency data\n Analysis of Extent of Cheating data\n HAPPE-ing: __H__ypothesing __A__fter __P__ost __P__ublication __E__valuation  The R code used to generate the results (and this page) is available in this Markdown file, and this post explains how to post to a WordPress blog.\n  I. Analysis of proportion / frequency data Some concerns can be raised about the significant differences between various conditions in proportion Cheating reported in the 4 experiments.\nFirst and foremost, no corrections for multiple comparisons are conducted, should one do so, just 2 significant proportion differences remain:\nMoney vs. Time in experiment 1 \u0026amp; 4. In Experiment 3, the sample difference No Mirror: Money - Time was marginally significant in the 2^nd significant digit (original: p = 0.015, adjusted \\(\\alpha\\) = 0.013, Bonferroni).\nSecond, no continuity correction is applied, these proportions are calculated from discrete numbers (participants). If a continuity correction is applied, 2-3 significant differences remain, depending on the \\(\\alpha\\)-level chosen:\n  Exp. Contrast Published Continuity corrected Bonferroni adjusted    1 Money-Time \u0026lt;.001 410^{-4} \u0026lt; 0.0167  1 Money-Ctrl \u0026lt;.05 0.0894 \u0026gt; 0.0167  1 Time-Ctrl \u0026lt;.05 0.0836 \u0026gt; 0.0167        2 Int: Money-Time \u0026lt;.01 0.1493 ~ 0.0125  2 Per: Money-Time \u0026gt;.05 1 \u0026gt; 0.0125  2 Money: Int-Per \u0026lt;.03 0.0856 \u0026gt; 0.0125  2 Time: Int-Per \u0026gt;.05 1 \u0026gt; 0.0125        3 Mir: Money-Time \u0026gt;.05 0.7996 \u0026gt; 0.0125  3 NoM: Money-Time \u0026lt;.003 0.0293 ~ 0.0125  3 Money: Mir-NoM \u0026gt;.05 0.0537 \u0026gt; 0.0125  3 Time: Mir-NoM \u0026gt;.05 1 \u0026gt; 0.0125        4 Money-Time \u0026lt;.001 10^{-4} \u0026lt; 0.0167  4 Money-Ctrl \u0026lt;.05 0.0522 \u0026gt; 0.0167  4 Time-Ctrl \u0026lt;.05 0.0752 \u0026gt; 0.0167               Number sig. results 9 3 Original: 4, Continuity: 2    This calls for a more appropriate analysis of frequency data:\nLog-linear analysis of observed cell frequencies\n Exact odds ratios of 2x2 sub-tables to test hypotheses using Effect Size CIs  (Cheating can be considered a dichotomous response, so logistic regression could also be used, see III. HAPPE-ing)\n Note:\nExperiment 2 \u0026amp; 3 do not list n per condition, the most likely values for n (1. closest to an integer value; 2. as equal as possible; 3. Add to total N) are assumed:\nExperiment 2\n    Prime Assessment Ncond * %Cheat = Ncheat (deviation)    Money Personality 36 * 0.2778 = 10.0008 (810^{-4})  Time Personality 35 * 0.2857 = 9.9995 (510^{-4})  Money Intelligence 38 * 0.5 = 19 (0)  Time Intelligence 33 * 0.303 = 9.999 (1010^{-4})    Experiment 3\n    Prime Assessment Ncond * %Cheat = Ncheat (deviation)    Money Mirror 31 * 0.387 = 11.997 (0.003)  Time Mirror 28 * 0.321 = 8.988 (0.012)  Money No Mirror 30 * 0.667 = 20.01 (0.01)  Time No Mirror 31 * 0.355 = 11.005 (0.005)     1. log-linear analysis of observed cell frequencies Log-linear analysis, or poisson regression using the generalised linear model, can be used to test whether relationships exist among the variables in a multi-way contingency table. Here I analyse the number of participants in each cell of the design: The observed frequencies take the role of the dependent variable and the levels of the design factors such as Mediator, Prime and Cheating are considered the levels of independent variables (another option would have been a logistic / probit regression with Cheating as the dependent binary / proportion variable).\nTwo types of result given for each experiment:\nFirst, a table listing deviance tests for the full (saturated) model. The analysis starts with the NULL model (all frequencies are equal) in the first row. Each subsequent row lists what happens to the deviance (of the model in the previous row) when a factor is added. A significant drop in deviance means adding the factor to the model contributes to predicting the difference between expected and observed frequencies. For hints of corroboration of the hypotheses reported in the paper, significant interactions between a design factor and Cheating are necessary.\nSecond, a mosaic plot is displayed, this is a graphical representation of the conditional cell frequencies. The mosaic plot also indicates which residual frequencies (observed - expected) are significantly below (red) or above (blue) the expected frequencies (residuals are interpretable as a Z-score). The coloured cells contribute most to a high and possibly significant \\(\\chi^2\\) value.\n Note: The significance of the change in deviance can depend on the order in which factors are added to the model and is not the same as a significant beta weight in a regression model.\n \u0026gt; [1] \u0026quot;Experiment 1\u0026quot; \u0026gt; Analysis of Deviance Table \u0026gt; \u0026gt; Model: poisson, link: log \u0026gt; \u0026gt; Response: Count \u0026gt; \u0026gt; Terms added sequentially (first to last) \u0026gt; \u0026gt; \u0026gt; Df Deviance Resid. Df Resid. Dev Pr(\u0026gt;Chi) \u0026gt; NULL 5 24.767 \u0026gt; Cheating 1 9.3328 4 15.434 0.0022509 ** \u0026gt; Prime 2 0.0205 2 15.414 0.9898129 \u0026gt; Cheating:Prime 2 15.4136 0 0.000 0.0004497 *** \u0026gt; --- \u0026gt; Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 \u0026gt; [1] \u0026quot;Experiment 2\u0026quot; \u0026gt; Analysis of Deviance Table \u0026gt; \u0026gt; Model: poisson, link: log \u0026gt; \u0026gt; Response: Count \u0026gt; \u0026gt; Terms added sequentially (first to last) \u0026gt; \u0026gt; \u0026gt; Df Deviance Resid. Df Resid. Dev Pr(\u0026gt;Chi) \u0026gt; NULL 7 19.6365 \u0026gt; Cheating 1 13.8608 6 5.7757 0.0001969 *** \u0026gt; Prime 1 0.2536 5 5.5221 0.6145539 \u0026gt; Test 1 0.0000 4 5.5221 1.0000000 \u0026gt; Cheating:Prime 1 1.5057 3 4.0164 0.2197998 \u0026gt; Cheating:Test 1 2.5348 2 1.4816 0.1113599 \u0026gt; Prime:Test 1 0.0307 1 1.4509 0.8609311 \u0026gt; Cheating:Prime:Test 1 1.4509 0 0.0000 0.2283780 \u0026gt; --- \u0026gt; Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 \u0026gt; [1] \u0026quot;Experiment 3\u0026quot; \u0026gt; Analysis of Deviance Table \u0026gt; \u0026gt; Model: poisson, link: log \u0026gt; \u0026gt; Response: Count \u0026gt; \u0026gt; Terms added sequentially (first to last) \u0026gt; \u0026gt; \u0026gt; Df Deviance Resid. Df Resid. Dev Pr(\u0026gt;Chi) \u0026gt; NULL 7 11.4971 \u0026gt; Cheating 1 2.1397 6 9.3574 0.14353 \u0026gt; Prime 1 0.0333 5 9.3241 0.85513 \u0026gt; Test 1 0.0333 4 9.2907 0.85513 \u0026gt; Cheating:Prime 1 4.2369 3 5.0538 0.03955 * \u0026gt; Cheating:Test 1 2.8451 2 2.2086 0.09165 . \u0026gt; Prime:Test 1 0.4973 1 1.7113 0.48069 \u0026gt; Cheating:Prime:Test 1 1.7113 0 0.0000 0.19081 \u0026gt; --- \u0026gt; Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 \u0026gt; [1] \u0026quot;Experiment 4\u0026quot; \u0026gt; Analysis of Deviance Table \u0026gt; \u0026gt; Model: poisson, link: log \u0026gt; \u0026gt; Response: Count \u0026gt; \u0026gt; Terms added sequentially (first to last) \u0026gt; \u0026gt; \u0026gt; Df Deviance Resid. Df Resid. Dev Pr(\u0026gt;Chi) \u0026gt; NULL 5 21.269 \u0026gt; Cheating 1 4.2195 4 17.049 0.0399621 * \u0026gt; Prime 2 0.2876 2 16.762 0.8660723 \u0026gt; Cheating:Prime 2 16.7615 0 0.000 0.0002292 *** \u0026gt; --- \u0026gt; Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Conclusion log-linear analysis:\nThis alternative, and in my opinion more appropriate analysis is in agreement with the results after correction for multiple comparisons and continuity:\n- The mosaic plots show that there may be some unexpected factors driving the “effects” reported in the paper: * In experiment 1 \u0026amp; 4 it is not so much the observed frequency of people that did cheat, but the number of participants that did not cheat that deviate from the expected frequencies based on table margins.\n* The Money prime caused less people to NOT cheat, whereas the Time prime caused more people to NOT cheat - If there is a difference in amount of Cheating between samples, it is likely a “main effect” between the Time and Money prime (Cheating:Prime interaction), it is found to cause a significant drop in deviance in Experiments 1, 3 and 4. - Experiment 2 stands out, because observed differences in Cheating are unlikely due to chance, but none of the other factors contribute to explain differences between expected and observed frequencies.\nThe point about the mosaic plots is not just semantics or methodologists’ nit-picking. What it tells us is that, e.g. in the mosaic plot Table.1.1, among the observed frequencies of CheatYES, the cell Money does not stand out much from Time and Control from what may be expected by chance, for CheatNO on the other hand, the cell Money does stand out as different.\n 2. Exact odds ratios of 2x2 subtables to test hypotheses using Effect Size CIs Effect Size Confidence Intervals:\nTo get a clearer idea about the significance between cell differences I calculate confidence intervals around the effect size associated with contingency tables. The CIs in Figure 1 below are based on the exact Odds Ratio (using the non-central hypergeomteric distribution) for a 2x2 sub-table of the full design obtained from Fisher's Exact Test, testing against $ H_0: OR = 1 $.\n\u0026gt; [1] \u0026quot;Figure 1. Exact log Odds Ratio\u0026#39;s of 2x2 tables comparing frequency of Cheating between independent samples in each experiment.\u0026quot;  Note:\nHere, the Confidence Levels have been adjusted to account for the fact that 3 (EXP1\u0026amp;4) and 4 (EXP2\u0026amp;3) subtables of the full design were compared (1-(0.05 / #tests)). The exact p-value from Fisher’s exact test reported in the Figure was multiplied by the number of comparisons in each experiment.\n Conclusion Proportion data  If there is an effect, it exists as a “main-effect” difference between the Money and Time primed samples in Experiment 1 and 4.\n Experiment 3 No Mirror: Money - Time is a marginal case. Experiment 2 did not yield any substantial effects.\n 4-5 out of 7 statistical inferences in the paper that are made based on proportion data should be considered invalid.     II. Analysis of extent of cheating The extent of Cheating concerns the difference between actual accuracy (which is not provided as a result) and reported accuracy by a participant.\nExperiment 1-3 report analyses of extent of Cheating including means and SD’s. Sample size assumptions for Experiments 2 and 3 are the same as above.\nCompare Cohen’s d CIs I created CIs around the effect sizes based on the means and SD reported for Experiment 1-3 using the R package MBESS.\n\u0026gt; [1] \u0026quot;Figure 2. Cohen\u0026#39;s d with exact CIs comparing extent of Cheating between independent samples in experiment 1-3.\u0026quot;  Conclusion Extent of Cheating The pattern is the same as the previous analyses: - Experiment 1 shows a clear effect between Money and Time samples\n- Experiment 3 No Mirror: Money - Time is again a close call\n  III. HAPPE-ing (Hypothesising After Post-Publication Evaluation) Should reviewers have noticed these issues with data analysis?\nYes, they should have!\nEven without re-analysing the published data as I have done here, the conclusions by the authors can be questioned based on a comparison of very elementary results:\n Across four experiments, using different primes and a variety of measures and tasks, we consistently\nfound that shifting people’s attention to time decreases dishonesty. Priming time makes people reflect\non who they are, and this self-reflection reduces their likelihood of behaving dishonestly.\n The clue is to compare the results across the 4 experiments and evaluate whether it is valid to infer that the core postulates have been corroborated. The designs and materials are slightly different each time, but if variation in outcomes (e.g., proportion cheating behaviour) varies systematically with one or more of the experimental differences, there may be another variable at work here.\nOne result that begs explanation is the drop in proportion Cheating in all the samples of Experiment 2 when compared to the other experiments. What is special about the procedure and methods? Regrettably more than 1 potential intervening factor changes with respect to Experiment 1.\nA second odd omission in the interpretation of the results is the level of accuracy achieved by participants. In Experiments 1-3, the urge to cheat must have been less when a participant had achieved 90% accuracy. Experiment 4 is somewhat different in that the cheating opportunity concerns one “bottleneck” problem that is difficult to solve, but has to be correct in order to make other more easily solvable problems count in adding to the final reward. Here, accuracy could have an opposite effect in which less accurate participants cheat less. If 0 or only 1 extra item past the “bottleneck” item were solved, a participant might be less inclined to cheat than a participant who solved every problem except for the “bottleneck” item.\nWhat is mediating what? The figure below shows the interaction between the maximal financial incentive that could be awarded and the proportion cheating for each prime and experimental condition (indicating whether a mediator variable was manipulated in addition to being exposed to a prime). Note that the Intelligence and the No Mirror condition of Experiments 2 and 3 respectively are considered similar to Experiment 1 and 4, that is, they reflect a condition in which Self-reflection was not induced by any other means than priming:\nThis relationship can be tested in a generalised linear model, of course being fully aware that this is exploratory HAPPE-ing. I assume the samples from each experiment are independent and use the number of cheaters vs. no cheaters as the dependent binomial variable. The model contains only those effects for which data are available (e.g., no interactions with both Prime and Mediator)\n Note:\nA generalised linear mixed model (GLMM) with sample ID as a random effect gives similar results.\n \u0026gt; \u0026gt; Call: \u0026gt; glm(formula = cbind(CheatYES, CheatNO) ~ Reward + Prime + Mediator + \u0026gt; Reward * Prime + Reward * Mediator, family = binomial, data = reward) \u0026gt; \u0026gt; Deviance Residuals: \u0026gt; Min 1Q Median 3Q Max \u0026gt; -1.1534 -0.6946 -0.1216 0.2508 1.9564 \u0026gt; \u0026gt; Coefficients: \u0026gt; Estimate Std. Error z value Pr(\u0026gt;|z|) \u0026gt; (Intercept) -0.44952 0.21947 -2.048 0.04054 * \u0026gt; Reward 0.01107 0.02186 0.506 0.61253 \u0026gt; PrimeNone 0.58682 0.39730 1.477 0.13967 \u0026gt; PrimeMoney 0.60398 0.28243 2.139 0.03247 * \u0026gt; MediatorSelf-reflection -0.81281 0.31474 -2.583 0.00981 ** \u0026gt; Reward:PrimeNone 0.01672 0.03593 0.465 0.64158 \u0026gt; Reward:PrimeMoney 0.06976 0.03270 2.133 0.03291 * \u0026gt; Reward:MediatorSelf-reflection -0.01894 0.04340 -0.436 0.66257 \u0026gt; --- \u0026gt; Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 \u0026gt; \u0026gt; (Dispersion parameter for binomial family taken to be 1) \u0026gt; \u0026gt; Null deviance: 76.292 on 13 degrees of freedom \u0026gt; Residual deviance: 11.035 on 6 degrees of freedom \u0026gt; AIC: 82.478 \u0026gt; \u0026gt; Number of Fisher Scoring iterations: 4 \u0026gt; [1] \u0026quot;Null-model deviance test: p \u0026lt; 1.33525644154704e-11\u0026quot; In the table above the model Intercept corresponds to the odds of Cheating compared to the Null-model when the predictors have the values: Prime = Time, Mediator = None and Reward = 0. Compared to the overall probability of observing Cheating behaviour, it thus seems that when the Time prime is presented without an induction of Self-reflection and a financial reward incentive, the odds of Cheating drop.\nThis appears to be a corroboration of the second postulate, but note that in this analysis (just as in the previous analyses), there is no real difference between the Time prime and prime = None. The standard errors around these parameters are quite high. A clearer picture emerges when the Intercept is defined as Prime = None, Mediator = None and Reward = 0 and the Odds Ratios are compared (exponentiation of the parameter estimates):\n\u0026gt; [1] \u0026quot;Odds Ratios compared to Prime = None, with profile likelihood CI.95\u0026quot; \u0026gt; OR 2.5 % 97.5 % \u0026gt; (Intercept) 1.15 0.60 2.21 \u0026gt; Reward 1.03 0.97 1.09 \u0026gt; PrimeTime 0.56 0.25 1.21 \u0026gt; PrimeMoney 1.02 0.47 2.21 \u0026gt; MediatorSelf-reflection 0.44 0.24 0.81 \u0026gt; Reward:PrimeTime 0.98 0.92 1.05 \u0026gt; Reward:PrimeMoney 1.05 0.98 1.14 \u0026gt; Reward:MediatorSelf-reflection 0.98 0.90 1.07 The odds ratios in the table above are multiplicative changes to the Probability of Cheating = 1 when the predictor increases by 1 unit. So an OR \u0026lt; 1 will decrease the odds of observing Cheating behaviour and an OR \u0026gt; 1 will increase it. The 95% CIs are based on the profile likelihood and show that in most cases the effect covers a range below and above 1. The range for the effect of Self-Reflection is always below 1.\nOne can interpret the modelled relationship between these variables as follows:\n* There is a weak positive association between the Maximal Financial Reward and the Probability of Cheating\n* The association changes with the value of Prime, becoming stronger when Money is primed, weaker when Time is primed\n* The induction of Self-reflection does not cause the association to change, it changes the intercept, the base-line Probability of Cheating at Reward = 0\nA graphical representation of the model predictions more clearly reveals this relationship:\n Conclusions, Discussion and further HAPPE-ing The significant results between Time and Money in Experiments 1 and 4 probably arise due to the increase in Probability of Cheating when there is a financial reward and Money is primed.\n   It is unlikely there are any other “real” differences in these data except for the induction of Self-reflection: Model predictions show it decreases the Probability of Cheating by the same amount for different primes Note that there were no actual data points for None + Self-reflection\n  The missing predictors in the Probability of Cheating analysis are the actual and reported accuracy of the performance (amount of correctly solved problems and money received respectively). These values cannot be inferred from the extent of cheating analyses. It seems reasonable to assume in most experiments there was less incentive to engage in Cheating by participants who were more accurate.   This brings up the question of whether the effects are driven by some sort of Speed-Accuracy instruction: Naturally, Time = Money, but taking the time to solve the problems may lead to higher accuracy and less incentive to cheat, likewise a focus on getting as many answers as possible may introduce errors and promote cheating.  In science there is a moral obligation to do the best one can to be as accurate as possible and usually this means it is wise to be as modest as possible about ones’ scientific claims. I am not an expert in this field, but the sheer amount of questions that can be raised about the validity of the inferences made in this paper makes one wonder who the peers were that achieved consensus about the credibility of this research and what their area of expertise was.\nI am not saying this is irrelevant, or poor research; the two effects that survive the scrutiny of 3PR are certainly interesting. I am just a little worried this paper says more about the morality of contemporary scientific publishing than the scientific study of moral behaviour.\n  Some notes about this file:  This file was created using Markdown in RStudio: Unless otherwise indicated in the code blocks (e.g., by require), the basic R packages are used. All the analyses are based on results reported in the publication. The one true gospel on statistical inference does not exist and more than one approach to analyse these data may be defensible. Therefore: Please be aware these comments and suggestions reflect my own preferences and standards in these matters. If you feel I should change some of my preferences and/or standards please let me know, because I review and adjust them on a regular basis.   ","date":1425729600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1425729600,"objectID":"5e45fb1635cfab453f302a4e7006e3d3","permalink":"/post/2015-03-07-time-money-morality/","publishdate":"2015-03-07T12:00:00Z","relpermalink":"/post/2015-03-07-time-money-morality/","section":"post","summary":"A Post-Publication Peer-Review (3PR) of _Time, Money, and Morality_ (Gino \u0026 Mogilne, 2013).","tags":["HIBAR","3PR","Ordered Categorical"],"title":"Time = Money: The Morality of Being Accurate.","type":"post"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown\n 3-in-1: Create, Present, and Publish your slides\n Supports speaker notes\n Mobile friendly slides\n  Controls  Next: Right Arrow or Space\n Previous: Left Arrow\n Start: Home\n Finish: End\n Overview: Esc\n Speaker notes: S\n Fullscreen: F\n Zoom: Alt + Click\n PDF Export: E\n  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$\nf\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}}\n$$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne \nTwo \nThree \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS)\n weight: sets the order in which a fragment appears\n  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes\n Press S key to view\n   Themes  black: Black background, white text, blue links (default)\n white: White background, black text, blue links\n league: Gray background, white text, blue links\n beige: Beige background, dark text, brown links\n sky: Blue background, thin dark text, blue links\n   night: Black background, thick white text, orange links\n serif: Cappuccino background, gray text, brown links\n simple: White background, black text, blue links\n solarized: Cream-colored background, dark green text, blue links\n  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown\n 3-in-1: Create, Present, and Publish your slides\n Supports speaker notes\n Mobile friendly slides\n  Controls  Next: Right Arrow or Space\n Previous: Left Arrow\n Start: Home\n Finish: End\n Overview: Esc\n Speaker notes: S\n Fullscreen: F\n Zoom: Alt + Click\n PDF Export: E\n  Code Highlighting Inline code: variable","tags":null,"title":"Slides","type":"slides"}]